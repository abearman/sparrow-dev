Python:

### First need to calibrate the cameras, to obtain each 3x4 camera matrix.
Probably only need to do this once, as long as the cameras don't move, and you put the drone in the same location to start every time. Or you could calibrate based on pattern points.

cv2.calibrateCamera(objectPoints, imagePoints, imageSize): 
- objectPoints: a vector of vectors of calibration pattern points.
- imagePoints: vector of vectors of the projections of calibration pattern points.
- imageSize: size of the image used only to initialize the intrinsic camera matrix.
- Returns a 3x3 cameraMatrix, rvecs (output vector of rotation vectors for each pattern view), tvecs (output vector of translation vectors for each pattern view), and distortion coefficients

ProjectionMatrix = [cameraMatrix] * [R | t]
http://stackoverflow.com/questions/16101747/how-can-i-get-the-camera-projection-matrix-out-of-calibratecamera-return-value

So maybe put the drone in a little launching pad, that we know the (x, y, z) coordinates of, where the cameras can see it, so they can calibrate. 

### Second, can get the location of each point (of three) on the drone, at any given moment. 

cv2.triangulatePoints(projMatr1, projMatr2, projPoints1, projPoints2): takes in the 3x4 projection matrix of each (two) camera, and two 2xN arrays of feature points, and returns a 4xN array of reconstructed points in homogenous coordinates.

So it could take in the three points on the drone (either you could click on them, to start, or it could locate this pixel cloud, and choose the center pixel), get corresponding sets of three points in each camera. 
